IFT 6269: Probabilistic Graphical Models Fall 2016 
Lecture 2 ‚Äî September 6 
Lecturer: 
Simon 
Lacoste-Julien 
Scribe: 
William 
L√©chelle 


Disclaimer: These notes have only been lightly proofread. 

2.1 Probability review 
2.1.1 Motivation 
Question : Why do we use probability in data science ? 
Answer : Probability theory is a principled framework to model uncertainty. 

Question : Where does uncertainty come from ? 
Answer : There are several sources : 

1. it can be intrinsic to certain phenomenon (e.g. quantum mechanics) ; 
2. reasoning about future events ; 
3. we can only get partial information about some complex phenomenon : 
(a) e.g. throwing a dice, it is hard to fully observe the initial conditions ; 
(b) for an object recognition model, a mapping from pixels to objects can be incredibly 
complex. 
2.1.2 Notation 
Note that probability theorists and the graphical models community both use a lot of notational
 shorthands. The meaning of notations often has to be inferred from the context. 
Therefore, let‚Äôs recall a few standard notations. 

Random variables will be noted X1,X2,X3,... , or sometimes X, Y, Z. Usually, they will 
be real-valued. 

x1,x2,x3,... (or x, y, z), will denote the realizations of the former random variables (the 
values the Xs can take). 

2-1 


Lecture 2 ‚Äî September 6 Fall 2016 

! R
X(!) 
X R
Y (!) 
Formally 

Let us define , a sample space of elementary 
events, {!1,!2,!3,... }1. 

Then a random variable is a (measurable2) mapping
 X : 7! R. 

Then, a probability distribution P is a mapping 

P : E 7! [0, 1], where E is the set of all subsets of 
, i.e. the set of events (i.e. 2 , i.e. a -field3) ; such that 
‚Äúmeasurements‚Äù

‚Äúworld of possibilities‚Äù

9 

‚àíP (E)  0 8E 2E 

>

= 

‚àíP (
) = 1 

11 Kolmogorov axioms 
‚àíP (
[ 
Ei)= X
(Ei) when E1,E2,... are disjoint. >; 

i=1 i=1 

Therefore, a probability distribution on induces a probability distribution on the image 
of X4 
: X 
, 
X(
). An event {x} for x 2 X 
thus gets the probability 

PX 
({x})= P({! : X(!)= x}) 
= P(X‚àí1({x})) 
= P{X = x} (shorthand) 
= p(x) actually used shorthand, even more ambiguous 

where X‚àí1(A) , 
{! : X(!) 2 A}. 

Example 

In the case of a dice roll, = {1, 2,..., 6}. Let‚Äôs consider two random variables : 
X measures whether the dice result is even. 
Y measures whether the dice result is odd. 

Formally, X = 1{2,4,6}, and Y = 1{1,3,5} where 

( 

1 if ! 2 A 

1A(!) , 


0 otherwise 

is the indicator function on A. 

1temporarily assumed to be a countable set 

2Wikipedia 

3the -field formalism is necessary when is uncountable, which happens as soon as we consider a 
continuous random variable. 

4The image of X is the set of the possible outputs of X : X(
) = {x : 9! 2 s.t. X(!)= x} 

2-2 


Lecture 2 ‚Äî September 6 Fall 2016 

13 
2 
4 
5 
6 
XY{X = 0, Y = 1} 
0 
1 (X(5), Y (5)) 
1 
(X, Y ) 
We can now define the joint distribution on (X, Y ) 2 X 
√ó Y 
. 

 

PX,Y 
({X = x,5 
Y = y})= PX‚àí1({x}) \ Y ‚àí1({y}) 

(X, Y ) can be called a random vector, or a vector-valued random variable, with ‚Äúrandom 
variable‚Äù meant in a generalized sense. 
We can represent the joint distribution as a table, such as in our running example : 

X =0 X =1 
Y =0 

0 121

Y =1 

2 0 

X 111 1

For instance : P ({X =1,Y =0})= P ({2, 4, 6})= p(!)=6+6+6= 2. 

!2{2,4,6}

Let‚Äôs also define, in the context of a joint distribution, the marginal distribution, i.e. the 
distribution on components of the random vector : 

X 

P {X = x} = P{X = x, Y = y} (sum rule) 

y2 Y 


This rule is a property, deriving it from the axioms is left as an exercice for the reader. 

2.1.3 Types of random variables 
Discrete random variables 

For a discrete random variable, X 
is countable. Its probability distribution on X 
, PX 
, is 
fully defined by its probability mass function (aka pmf), PX 
({X = x}), for x 2 X 
. This 
notation is shortened as PX 
(x), and even as p(x), ‚Äútyping‚Äù x as only denoting values of the 
X variable. Thereby, it is possible that p(x) 6= p(y) even if x = y, in the sense that p(x) 
means PX 
(x) and p(y) means PY 
(y). 

More generally, for X 
2 R, the probability distribution PX 
is fully characterized by its 
cumulative distribution function (aka cdf) : FX 
(x) , 
PX 
{X  x}. 

5This comma means and, the intersection of both events. 

2-3 


Lecture 2 ‚Äî September 6 Fall 2016 

It has the following properties : 

1. FX 
is non-decreasing ; 
2. lim FX 
(x)=0; 
x!‚àí1 

1 
FX 
(x) 
‚àí1 1 

3. lim FX 
(x) = 1. 
x!+1 

Example of a cumulative distribution 
function.

For discrete random variables, the cumulative distribution
 function is piecewise constant, and has jumps. 

Continuous random variables 

For a continuous random variable, the cumulative distribution function is ‚Äúabsolutely con-

x

tinuous‚Äù, i.e. is differentiable almost everywhere, and 9f(x) s.t. FX 
(x)= R 
‚àí1 f(u)du. Said 
f is called the probability density function of the random variable (aka pdf). Where f is 
continuous, dxd 
FX 
(x)= f(x). 

The probability density function is the continuous analog of the probability mass function 
of a discrete random variable (with sums becoming integrals). Hence : 

discrete continuous

Z

X 

p(x)=1 p(x)=1 

X

x2 

p = prob. X 
mass function p = prob. density function 

Note in the continuous case, as a density function, p(x) can be greater than 1, on a 
sufficiently narrow interval. For instance, the uniform distribution on [0, 21 ]: ( 1

2 for x 2 [0, 2 ]

p(x)= 

0 otherwise 

2.1.4 Other random variable basics 
Expectation/mean 

The expectation of a random variable is 

Z

X 

E[X] , 
xp(x) or xp(x) dx (in the continuous case) 

x2 

X 


X 


Variance 

V ar[X] , 
E[(X ‚àí E(X))2] 
= E[X2] ‚àí E[X]2 

Variance is a measure of the dispersion of values around the mean. 

2-4 


Lecture 2 ‚Äî September 6 Fall 2016 

Independance 

X is independant from Y , noted X ? Y , iff p(x, y)= p(x)p(y) 8x, y 2 X 
√ó Y 
.

Q

n

Random variables X1,...Xn 
are mutually independant iff p(x1,...xn)= i=1 p(xi). 

Conditioning 

For events A and B, suppose that p(B) 6= 0. We define the probability of A given B, 

P(A \ B)

P(A|B) , 


P(B) 

In terms of sample space, that means we look at the subspace where B happens, and in 
that space, we look at the subspace where A also happens. 

For random variables X and Y , thus : 

P (X = x, Y = y)

P (X = x|Y = y) , 


P(Y = y) 
P(Y = y)= P 
x 
P (X = x, Y = y) is a normalization constant, necessary in order to get a 
real probability distribution. 
By definition, we get the product rule : 
p(x, y)= p(x|y)p(y) (product rule) 
It is always true, with the subtle point that p(x|y) is undefined if p(y) = 0.6 


Bayes rule 

Bayes rule is about inverting the conditioning of the variables. 

p(y|x)p(x) p(y|x)p(x) 

p(x|y)= = P (Bayes rule)

0

p(y) x0 
p(x,y) 

Chain rule 

By successive application of the product rule, it is always true that : 

p(x1,...,xn)= p(x1:n‚àí1)p(xn|x1:n‚àí1) 
= ¬∑¬∑¬∑ (Chain rule)

Q

n

= i=1 p(xi|x1,...,xi‚àí1) 

The last part can be simplified using the conditional independance asumptions we make, 
like in the case of directed graphical models. 

6In probability theory, we usually do not care what happens on sets with probability zero; so we are free 
to define p(x|y) to be any value we want when p(y) = 0. 

2-5 


Lecture 2 ‚Äî September 6 Fall 2016 

Conditional independance 

X is conditionally independant of Y given Z, noted X ? Y |Z, iff 

p(x, y|z)= p(x|z)p(y|z) 8x, y, z 2 x√ó y√ó z 
s.t. p(z) 6=0 

For instance, with Z the probability that a mother carries a genetic disease on chromosome
 X, X the probability for her first child to carry the disease, and Y the same probability 
for her second child, we can say that X is independant of Y given Z (because only the status 
of the mother impacts directly each child : once that is known, children‚Äôs probabilities of 
carrying the disease are independant from each other). 

As an exercise to the reader, prove that p(x|y, z)= p(x|z) when X ? Y |Z. 

2-6 


