Tokenization:
----
Tokens are the building blocks of Natural Language. Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types â€“ word, character, and subword (n-gram characters) tokenization.

Similarly, tokens can be either characters or subwords. 


For example, let us consider **smarter**:

1. `Character tokens`: s-m-a-r-t-e-r
2. `Subword tokens`: smart-er

But then is this necessary? Do we really need tokenization to do all of this?
